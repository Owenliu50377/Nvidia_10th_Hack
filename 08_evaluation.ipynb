{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35536f6-166c-4b89-8136-96417db5be30",
   "metadata": {
    "id": "b35536f6-166c-4b89-8136-96417db5be30"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 8 [Assessment]:** RAG Evaluation</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "Welcome to the last notebook of the course! In the previous notebook, you integrated a vector store solution into a RAG pipeline! In this notebook, you will take that same pipeline and evaluate it using numerical RAG evaluation techniques incorporating LLM-as-a-Judge metrics!\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Learn how to integrate the techniques from prior notebooks to numerically approximate the goodness of your RAG pipeline.\n",
    "\n",
    "- **Final Exercice**: ***By working through this notebook in the Course Environment,* you will be able to submit the coding component of the course!**\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- As you go along, remember what our metrics actually represent. Should our pipeline pass these objectives? Is our judge LLM sufficient for evaluating the pipeline? Does a particular metric even matter for our use case?\n",
    "- If we left the vectorstore-as-a-memory component in our chain, do you think it would still pass the evaluation? Additionally, is the evaluation useful for assessing vectorstore-as-a-memory performance? \n",
    "\n",
    "<br>\n",
    "\n",
    "### **Notebook Source:**\n",
    "\n",
    "- This notebook is part of a larger [**NVIDIA Deep Learning Institute**](https://www.nvidia.com/en-us/training/) course titled [**Building RAG Agents with LLMs**](https://www.nvidia.com/en-sg/training/instructor-led-workshops/building-rag-agents-with-llms/). If sharing this material, please give credit and link back to the original course.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "w_A3rZOrIeQD",
   "metadata": {
    "id": "w_A3rZOrIeQD"
   },
   "outputs": [],
   "source": [
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu ragas\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "norm_style = Style(bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "pprint2 = partial(console.print, style=norm_style)\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "NVIDIAEmbeddings.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "embedder = NVIDIAEmbeddings(\n",
    "    model=\"nvidia/embed-qa-4\", truncate=\"END\",\n",
    "    base_url=\"http://llm_client:9000/v1\"\n",
    ")\n",
    "\n",
    "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "instruct_llm = ChatNVIDIA(\n",
    "    model=\"mistralai/mixtral-8x22b-instruct-v0.1\",\n",
    "    base_url=\"http://llm_client:9000/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zEgV11oZmJGg",
   "metadata": {
    "id": "zEgV11oZmJGg"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Pre-Release Evaluation\n",
    "\n",
    "In our previous notebook, we successfully combined several concepts to create a document chatbot with the aim of responsive and informative interactions. However, the diversity of user interactions necessitates comprehensive testing to truly understand the chatbot's performance. Thorough testing in varied scenarios is crucial to ensure that the system is not only robust and versatile but also aligns with user and provider expectations.\n",
    "\n",
    "After defining your chatbot's roles and implementing the necessary features, evaluating it becomes a multi-stage process:\n",
    "\n",
    "- **Typical Use Inspection:** Start by testing scenarios most relevant to your use case. See if your chatbot can reliably navigate discussions with limited human intervention.\n",
    "\n",
    "    - Additionally, identify limitations or compartments that should be redirected to a human for inspection/supervision (i.e., human swap-in to confirm transactions or perform sensitive navigation) and implement those options.\n",
    "\n",
    "- **Edge Case Inspection:** Explore the boundaries of typical use, identifying how the chatbot handles less common but plausible scenarios.\n",
    "\n",
    "    - Before any public release, assess critical boundary conditions that could pose liability risks, such as the potential generation of inappropriate content.\n",
    "\n",
    "    - Implement well-tested guardrails on all outputs (and possibly inputs) to limit undesired interactions and redirect users into predictable conversation flows.\n",
    "\n",
    "- **Progressive Rollout:** Rolling out your model to a limited audience (first internal, then [A/B](https://en.wikipedia.org/wiki/A/B_testing)) and implement analytics features like usage analytics dashboards and feedback avenues (flag/like/dislike/etc).\n",
    "\n",
    "Of these three steps, the first two can be done by a small team or an individual and should be iterated on as part of the development process. Unfortunately, this needs to be done frequently and can be prone to human error. **Luckily for us, LLMs can be used to help out with LLM-as-a-Judge formulations!**\n",
    "\n",
    "*(Yeah, this probably isn't surprising by now. LLMs being strong is why this course is here...).*\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** LLM-as-a-Judge Formulation\n",
    "\n",
    "In the realm of conversational AI, using LLMs as evaluators or 'judges' has emerged as a useful approach for configurable automatic testing of natural language task performance:\n",
    "\n",
    "- An LLM can simulate a range of interaction scenarios and generate synthetic data, allowing an evaluation developer to generate targeted inputs to eliciting a range of behaviors from your chatbot.\n",
    "\n",
    "- The chatbot's correspondence/retrieval on the synthetic data can be evaluated or parsed by an LLM and a consistent output format such as \"Pass\"/\"Fail\", similarity, or extraction can be enforced.\n",
    "\n",
    "- Many such results can be aggregated and a metric can be derived which explains something like \"% of passing evaluations\", \"average number of relevant details from the sources\", \"average cosine similarity\", etc.\n",
    "\n",
    "This idea of using LLMs to test out and quantify chatbot quality, known as [**\"LLM-as-a-Judge,\"**](https://arxiv.org/abs/2306.05685) allows for easy test specifications that align closely with human judgment and can be fine-tuned and replicated at scale.\n",
    "\n",
    "**There are several popular frameworks for off-the-shelf judge formulations including:**\n",
    "- [**RAGAs (short for RAG Assessment)**](https://docs.ragas.io/en/stable/), which offers a suite of great starting points for your own evaluation efforts.\n",
    "- [**LangChain Evaluators**](https://python.langchain.com/docs/guides/evaluation/), which are similar first-party options with many implicitly-constructible agents.\n",
    "\n",
    "Instead of using the chains as-is, we will instead expand on the ideas and evaluate our system with a more custom solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDDNaBA9N3XM",
   "metadata": {
    "id": "fDDNaBA9N3XM"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3: [Assessment Prep]** Pairwise Evaluator\n",
    "\n",
    "The following exercise will flesh out a custom implementation of a simplified [LangChain Pairwise String Evaluator](https://python.langchain.com/docs/guides/evaluation/examples/comparisons). \n",
    "\n",
    "**To prepare for our RAG chain evaluation, we will need to:**\n",
    "\n",
    "- Pull in our document index (the one we saved in the previous notebook).\n",
    "- Recreate our RAG pipeline of choice.\n",
    "\n",
    "**We will specifically be implementing a judge formulation with the following steps:**\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "**The chain should be a simple but powerful process that tests for the following objective:**\n",
    "\n",
    "> ***Does my RAG chain outperform a narrow chatbot with limited document access.***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**This will be the system used for the final evaluation!** To see how this system is integrated into the autograder, please check out the implementation in [`frontend/server_app.py`](frontend/server_app.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bh8jaOqak0f",
   "metadata": {
    "id": "1bh8jaOqak0f"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 1:** Pull In Your Document Retrieval Index\n",
    "\n",
    "For this exercise, you will pull in the `docstore_index` file you created as part of your earlier notebook. The following cell should be able to load in the store as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "tlE7a2lseLOy",
   "metadata": {
    "id": "tlE7a2lseLOy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Constructed aggregate docstore with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">313</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> chunks</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mConstructed aggregate docstore with \u001b[0m\u001b[1;36m313\u001b[0m\u001b[1;38;2;118;185;0m chunks\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sample Chunk:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSample Chunk:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning\n",
      "\n",
      "Summary: Huge language models (LMs) have ushered in a new era for AI, serving as a\n",
      "gateway to natural-language-based knowledge tasks. Although an essential\n",
      "element of modern AI, LMs are also inherently limited in a number of ways. We\n",
      "discuss these limitations and how they can be avoided by adopting a systems\n",
      "approach. Conceptualizing the challenge as one that involves knowledge and\n",
      "reasoning in addition to linguistic processing, we define a flexible\n",
      "architecture with multiple neural models, complemented by discrete knowledge\n",
      "and reasoning modules. We describe this neuro-symbolic architecture, dubbed the\n",
      "Modular Reasoning, Knowledge and Language (MRKL, pronounced \"miracle\") system,\n",
      "some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs'\n",
      "MRKL system implementation.\n",
      "\n",
      "Page Body: . We further observe that training\\n9\\non words generalizes well to digits. Interestingly, the inverse is not true and the per-\\nformance is much lower, indicating the underlying di\\ufb00erence in the representations\\nof the numbers.\\nTest\\nTrain\\nDigits\\nWords\\nDigits\\n1.0\\n0.156\\nWords\\n0.987\\n0.988\\nTable 2:\\nRobustness to wordings. Accuracy for all combination of training and testing\\non numbers written as digits and as words. Results are averaged across all the numbers of\\ndigits.\\nExperiment 3: Generalization across question formats.\\nA main challenge\\nin constructing natural language interfaces to discrete reasoners like a calculator is in\\nhandling language variability. One of the most appealing characteristics of language\\nmodels is their ability to abstract away from such variability. Here we test the ability\\nof the model to generalize over formats of arithmetic problems. Experiments are for\\nsingle-operation problems\n"
     ]
    }
   ],
   "source": [
    "## Make sure you have docstore_index.tgz in your working directory\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/embed-qa-4\", truncate=\"END\")\n",
    "\n",
    "!tar xzvf docstore_index.tgz\n",
    "docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "def format_chunk(doc):\n",
    "    return (\n",
    "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
    "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
    "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
    "    )\n",
    "\n",
    "## This printout just confirms that your store has been retrieved\n",
    "pprint(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "pprint(f\"Sample Chunk:\")\n",
    "print(format_chunk(docs[len(docs)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dib0F-t2N4LJ",
   "metadata": {
    "id": "dib0F-t2N4LJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 2: [Exercise]** Pull In Your RAG Chain\n",
    "\n",
    "Now that we have our index, we can recreate the RAG agent from the previous notebook! \n",
    "\n",
    "**Key Modifications:**\n",
    "- To keep things simple, feel free to disregard the vectorstore-as-a-memory component. Incorporating it will require some more overhead and will make the exercise a bit more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "XBi6Y8b8aXd2",
   "metadata": {
    "id": "XBi6Y8b8aXd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document discusses a paper that makes two significant contributions: first, it presents a systematic study on LLM-as-a-judge; second, it introduces human preference datasets with high-quality questions and varied user interactions from MT-bench and Chatbot Arena. Interestingly, the paper argues for the adoption of a hybrid evaluation framework for future LLM benchmarks, combining existing capability-based benchmarks and new preference-based benchmarks with LLM-as-a-judge. This will enable swift and automatic evaluation of both the core capabilities and human alignment of models. The paper also released 80 MT-bench questions, 3K expert votes, and 30K conversations with human preferences for future study. [Source: Quote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena]\n",
      "\n",
      "Another interesting point is that a recently introduced model called REALM combined masked language models with a differentiable retriever, showing promising results for knowledge-intensive NLP tasks. This work has several positive societal benefits, such as generating responses that are more factual, offering more control and interpretability. The model could be employed in various scenarios with direct benefits to society, for example, by endowing it with a medical index and asking open-domain questions on the topic. However, the model's downside is that Wikipedia or any external knowledge source might never be completely factual and devoid of bias. [Source: Quote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks]"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "embedder = NVIDIAEmbeddings(\n",
    "    model=\"nvidia/embed-qa-4\", truncate=\"END\",\n",
    "    base_url=\"http://llm_client:9000/v1\"\n",
    ")\n",
    "\n",
    "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "instruct_llm = ChatNVIDIA(\n",
    "    model=\"mistralai/mixtral-8x22b-instruct-v0.1\",\n",
    "    base_url=\"http://llm_client:9000/v1\"\n",
    ")\n",
    "llm = instruct_llm | StrOutputParser()\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
    "    \" The following information may be useful for your response: \"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    ")\n",
    "\n",
    "def output_puller(inputs):\n",
    "    \"\"\"\"Output generator. Useful if your chain returns a dictionary with key 'output'\"\"\"\n",
    "    if isinstance(inputs, dict):\n",
    "        inputs = [inputs]\n",
    "    for token in inputs:\n",
    "        if token.get('output'):\n",
    "            yield token.get('output')\n",
    "\n",
    "#####################################################################\n",
    "## TODO: Pull in your desired RAG Chain. Memory not necessary\n",
    "\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)  ## GIVEN\n",
    "# context_getter = RunnableLambda(lambda x: x)  ## TODO\n",
    "context_getter = itemgetter('input') | docstore.as_retriever() | long_reorder | docs2str\n",
    "retrieval_chain = {'input' : (lambda x: x)} | RunnableAssign({'context' : context_getter})\n",
    "\n",
    "## Chain Specs: retrieval_chain -> generator_chain\n",
    "##   -> {\"output\" : <str>, ...} -> output_puller\n",
    "# generator_chain = RunnableLambda(lambda x: x)  ## TODO\n",
    "generator_chain = chat_prompt | llm  ## TODO\n",
    "generator_chain = {\"output\" : generator_chain } | RunnableLambda(output_puller)  ## GIVEN\n",
    "\n",
    "## END TODO\n",
    "\n",
    "## END TODO\n",
    "#####################################################################\n",
    "\n",
    "rag_chain = retrieval_chain | generator_chain\n",
    "\n",
    "# pprint(rag_chain.invoke(\"Tell me something interesting!\"))\n",
    "for token in rag_chain.stream(\"Tell me something interesting!\"):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b880971-d3a0-433f-a60b-e8a4edb754c8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Step 3:** Generating Synthetic Question-Answer Pairs\n",
    "\n",
    "In this section, we can implement the first few part of our evaluation routine:\n",
    "\n",
    "- **Sample the RAG agent document pool to find two document chunks.**\n",
    "- **Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.**\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ymzuX-DSNvL6",
   "metadata": {
    "id": "ymzuX-DSNvL6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How does the Modular Reasoning, Knowledge and Language (MRKL) system and Bidirectional Encoder </span>\n",
       "<span style=\"font-weight: bold\">Representations from Transformers (BERT) differ in their approach to natural language processing tasks, and what </span>\n",
       "<span style=\"font-weight: bold\">are some of the achievements of BERT compared to previous models?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How does the Modular Reasoning, Knowledge and Language \u001b[0m\u001b[1m(\u001b[0m\u001b[1mMRKL\u001b[0m\u001b[1m)\u001b[0m\u001b[1m system and Bidirectional Encoder \u001b[0m\n",
       "\u001b[1mRepresentations from Transformers \u001b[0m\u001b[1m(\u001b[0m\u001b[1mBERT\u001b[0m\u001b[1m)\u001b[0m\u001b[1m differ in their approach to natural language processing tasks, and what \u001b[0m\n",
       "\u001b[1mare some of the achievements of BERT compared to previous models?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The MRKL system, as described in the first paper, is a neuro-symbolic architecture that combines large </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language models, external knowledge sources, and discrete reasoning to overcome the limitations of language models </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(LMs). The MRKL system is a flexible architecture with multiple neural models, complemented by discrete knowledge </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and reasoning modules. It is designed to handle knowledge and reasoning tasks in addition to linguistic processing.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">On the other hand, BERT, described in the second paper, is a conceptually simple yet empirically powerful language </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">representation model that pre-trains deep bidirectional representations from unlabeled text. By jointly </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">conditioning on both left and right context in all layers, the pre-trained BERT model can be fine-tuned to create </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art models for various tasks such as question answering and language inference without substantial </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">task-specific architecture modifications. BERT has achieved new state-of-the-art results on eleven natural language</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">processing tasks, including improvements on the GLUE score, MultiNLI accuracy, and SQuAD question answering.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: The MRKL system, as described in the first paper, is a neuro-symbolic architecture that combines large \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage models, external knowledge sources, and discrete reasoning to overcome the limitations of language models \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. The MRKL system is a flexible architecture with multiple neural models, complemented by discrete knowledge \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand reasoning modules. It is designed to handle knowledge and reasoning tasks in addition to linguistic processing.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mOn the other hand, BERT, described in the second paper, is a conceptually simple yet empirically powerful language \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrepresentation model that pre-trains deep bidirectional representations from unlabeled text. By jointly \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconditioning on both left and right context in all layers, the pre-trained BERT model can be fine-tuned to create \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstate-of-the-art models for various tasks such as question answering and language inference without substantial \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtask-specific architecture modifications. BERT has achieved new state-of-the-art results on eleven natural language\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprocessing tasks, including improvements on the GLUE score, MultiNLI accuracy, and SQuAD question answering.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How does the fabrication process of the single-layer pentagonal lattice-based photonic crystal reflector </span>\n",
       "<span style=\"font-weight: bold\">affect its thickness and dimensions, and how does this reflector enhance reflectivity and reduce mass compared to </span>\n",
       "<span style=\"font-weight: bold\">an unpatterned SiN membrane?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How does the fabrication process of the single-layer pentagonal lattice-based photonic crystal reflector \u001b[0m\n",
       "\u001b[1maffect its thickness and dimensions, and how does this reflector enhance reflectivity and reduce mass compared to \u001b[0m\n",
       "\u001b[1man unpatterned SiN membrane?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The single-layer pentagonal lattice-based photonic crystal reflector is fabricated using a process that </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">involves spin coating, soft baking, exposure, development, and etching steps. The fabricated reflector has </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dimensions of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> mm^</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> with a thickness of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> nm. It is perforated with over a billion nanoscale features, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">which enhances reflectivity and reduces mass compared to an unpatterned SiN membrane. Specifically, simulations </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">have shown that the pentagonal lattice design achieves a reflectivity of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% for an Area fraction of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">55</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% and a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">thickness of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.18</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> µm, compared to a </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% reflective and </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> nm thick unpatterned SiN membrane. This is in agreement </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">with theoretical predictions.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: The single-layer pentagonal lattice-based photonic crystal reflector is fabricated using a process that \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minvolves spin coating, soft baking, exposure, development, and etching steps. The fabricated reflector has \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdimensions of \u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;38;2;118;185;0m x \u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;38;2;118;185;0m mm^\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m with a thickness of \u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;38;2;118;185;0m nm. It is perforated with over a billion nanoscale features, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwhich enhances reflectivity and reduces mass compared to an unpatterned SiN membrane. Specifically, simulations \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhave shown that the pentagonal lattice design achieves a reflectivity of \u001b[0m\u001b[1;36m84\u001b[0m\u001b[1;38;2;118;185;0m% for an Area fraction of \u001b[0m\u001b[1;36m55\u001b[0m\u001b[1;38;2;118;185;0m% and a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthickness of \u001b[0m\u001b[1;36m0.18\u001b[0m\u001b[1;38;2;118;185;0m µm, compared to a \u001b[0m\u001b[1;36m35\u001b[0m\u001b[1;38;2;118;185;0m% reflective and \u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;38;2;118;185;0m nm thick unpatterned SiN membrane. This is in agreement \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwith theoretical predictions.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How does the Mistral 7B language model's performance compare to other models such as Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> 13B and </span>\n",
       "<span style=\"font-weight: bold\">Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> 34B? Additionally, how have innovations in neural topology optimization contributed to the development of a</span>\n",
       "<span style=\"font-weight: bold\">scalable and cost-effective solution for lightsail materials in the Starshot Breakthrough Initiative?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How does the Mistral 7B language model's performance compare to other models such as Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m 13B and \u001b[0m\n",
       "\u001b[1mLlama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m 34B? Additionally, how have innovations in neural topology optimization contributed to the development of a\u001b[0m\n",
       "\u001b[1mscalable and cost-effective solution for lightsail materials in the Starshot Breakthrough Initiative?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The Mistral 7B language model outperforms the best open 13B model (Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) across all evaluated benchmarks</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and the best released 34B model (Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) in reasoning, mathematics, and code generation. The Mistral 7B uses </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">grouped-query attention for faster inference and sliding window attention for effectively handling sequences of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">arbitrary length with reduced inference cost. In the context of the Starshot Breakthrough Initiative, neural </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">topology optimization has led to the development of a novel pentagonal lattice-based photonic crystal reflector for</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">lightsails. These optimized designs not only shorten acceleration times, lowering launch costs, but also enable </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">lightsail material fabrication with orders-of-magnitude cost reduction. A </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> mm2, 200nm thick, single-layer </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reflector has been fabricated with over a billion nanoscale features, achieving nearly </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> times cost reduction </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">per m2, making it the highest aspect-ratio nanophotonic element to date.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: The Mistral 7B language model outperforms the best open 13B model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLlama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m across all evaluated benchmarks\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand the best released 34B model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLlama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m in reasoning, mathematics, and code generation. The Mistral 7B uses \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgrouped-query attention for faster inference and sliding window attention for effectively handling sequences of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0marbitrary length with reduced inference cost. In the context of the Starshot Breakthrough Initiative, neural \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtopology optimization has led to the development of a novel pentagonal lattice-based photonic crystal reflector for\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlightsails. These optimized designs not only shorten acceleration times, lowering launch costs, but also enable \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlightsail material fabrication with orders-of-magnitude cost reduction. A \u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;38;2;118;185;0m x \u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;38;2;118;185;0m mm2, 200nm thick, single-layer \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreflector has been fabricated with over a billion nanoscale features, achieving nearly \u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\u001b[1;36m000\u001b[0m\u001b[1;38;2;118;185;0m times cost reduction \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mper m2, making it the highest aspect-ratio nanophotonic element to date.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_questions = 3\n",
    "synth_questions = []\n",
    "synth_answers = []\n",
    "\n",
    "simple_prompt = ChatPromptTemplate.from_template('INSTRUCTION:\\n\\n{system}\\n\\nINPUT:\\n\\n{input}')\n",
    "\n",
    "for i in range(num_questions):\n",
    "    doc1, doc2 = random.sample(docs, 2)\n",
    "    sys_msg = (\n",
    "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
    "        \" Try to use both documents if possible, and rely more on the document bodies than the summary.\"\n",
    "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
    "    )\n",
    "    usr_msg = (\n",
    "        f\"Document1: {format_chunk(doc1)}\\n\\n\"\n",
    "        f\"Document2: {format_chunk(doc2)}\"\n",
    "    )\n",
    "\n",
    "    qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
    "    synth_questions += [qa_pair.split('\\n\\n')[0]]\n",
    "    synth_answers += [qa_pair.split('\\n\\n')[1]]\n",
    "    pprint2(f\"QA Pair {i+1}\")\n",
    "    pprint2(synth_questions[-1])\n",
    "    pprint(synth_answers[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5Q-3X4vS98P",
   "metadata": {
    "id": "c5Q-3X4vS98P"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 4:** Answer The Synthetic Questions\n",
    "\n",
    "In this section, we can implement the third part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- **Use the RAG agent to generate its own answer.**\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7T3GSwhZPHjF",
   "metadata": {
    "id": "7T3GSwhZPHjF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"font-weight: bold\">Question: How does the Modular Reasoning, Knowledge and Language (MRKL) system and Bidirectional Encoder </span>\n",
       "<span style=\"font-weight: bold\">Representations from Transformers (BERT) differ in their approach to natural language processing tasks, and what </span>\n",
       "<span style=\"font-weight: bold\">are some of the achievements of BERT compared to previous models?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\u001b[1mQuestion: How does the Modular Reasoning, Knowledge and Language \u001b[0m\u001b[1m(\u001b[0m\u001b[1mMRKL\u001b[0m\u001b[1m)\u001b[0m\u001b[1m system and Bidirectional Encoder \u001b[0m\n",
       "\u001b[1mRepresentations from Transformers \u001b[0m\u001b[1m(\u001b[0m\u001b[1mBERT\u001b[0m\u001b[1m)\u001b[0m\u001b[1m differ in their approach to natural language processing tasks, and what \u001b[0m\n",
       "\u001b[1mare some of the achievements of BERT compared to previous models?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: The Modular Reasoning, Knowledge and Language (MRKL) system and Bidirectional Encoder Representations </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">from Transformers (BERT) both play significant roles in natural language processing tasks but use different </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">approaches.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">BERT, as per the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, is a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language representation model that uses bidirectional transformers. This means it considers the context on both </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sides of a word, resulting in a deeper sense of language context. It's designed to pre-train deep bidirectional </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">representations from unlabeled text, which can then be fine-tuned with just one additional output layer to create </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art models for various tasks without significant task-specific architecture modifications. BERT has </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">achieved several milestones, including new state-of-the-art results on eleven natural language processing tasks. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">For example, it improved the GLUE score to </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80.5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% and pushed the SQuAD v1.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> question answering Test F1 to </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93.2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">On the other hand, the MRKL system is a neuro-symbolic architecture that combines large language models, such as </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">BERT, with external knowledge sources and discrete reasoning. The MRKL system aims to extend the capabilities of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language models by integrating symbolic reasoning and external knowledge, allowing it to solve tasks that BERT </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">alone cannot. It's worth noting that while BERT is a specific model, MRKL is more of an architectural pattern that </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">can use BERT as one of its components.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">These differences reflect a broader trend in AI research towards integrating neural and symbolic approaches to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">achieve more robust and versatile systems. It's also interesting to see how models like BERT are being incorporated</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">into these broader architectures like MRKL.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In summary, BERT represents a powerful pretrained language model that has achieved remarkable results in a variety </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of NLP tasks. MRKL, meanwhile, is a broader architectural concept that seeks to leverage the strengths of models </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">like BERT along with external knowledge and reasoning to tackle even more challenging tasks.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sources:</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sources and discrete reasoning]</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: The Modular Reasoning, Knowledge and Language \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMRKL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m system and Bidirectional Encoder Representations \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfrom Transformers \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mBERT\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m both play significant roles in natural language processing tasks but use different \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mapproaches.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mBERT, as per the paper \u001b[0m\u001b[32m\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"\u001b[0m\u001b[1;38;2;118;185;0m, is a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage representation model that uses bidirectional transformers. This means it considers the context on both \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msides of a word, resulting in a deeper sense of language context. It's designed to pre-train deep bidirectional \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrepresentations from unlabeled text, which can then be fine-tuned with just one additional output layer to create \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstate-of-the-art models for various tasks without significant task-specific architecture modifications. BERT has \u001b[0m\n",
       "\u001b[1;38;2;118;185;0machieved several milestones, including new state-of-the-art results on eleven natural language processing tasks. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mFor example, it improved the GLUE score to \u001b[0m\u001b[1;36m80.5\u001b[0m\u001b[1;38;2;118;185;0m% and pushed the SQuAD v1.\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m question answering Test F1 to \u001b[0m\u001b[1;36m93.2\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOn the other hand, the MRKL system is a neuro-symbolic architecture that combines large language models, such as \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mBERT, with external knowledge sources and discrete reasoning. The MRKL system aims to extend the capabilities of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage models by integrating symbolic reasoning and external knowledge, allowing it to solve tasks that BERT \u001b[0m\n",
       "\u001b[1;38;2;118;185;0malone cannot. It's worth noting that while BERT is a specific model, MRKL is more of an architectural pattern that \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcan use BERT as one of its components.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThese differences reflect a broader trend in AI research towards integrating neural and symbolic approaches to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0machieve more robust and versatile systems. It's also interesting to see how models like BERT are being incorporated\u001b[0m\n",
       "\u001b[1;38;2;118;185;0minto these broader architectures like MRKL.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn summary, BERT represents a powerful pretrained language model that has achieved remarkable results in a variety \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mof NLP tasks. MRKL, meanwhile, is a broader architectural concept that seeks to leverage the strengths of models \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlike BERT along with external knowledge and reasoning to tackle even more challenging tasks.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSources:\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mMRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msources and discrete reasoning\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "<span style=\"font-weight: bold\">Question: How does the fabrication process of the single-layer pentagonal lattice-based photonic crystal reflector </span>\n",
       "<span style=\"font-weight: bold\">affect its thickness and dimensions, and how does this reflector enhance reflectivity and reduce mass compared to </span>\n",
       "<span style=\"font-weight: bold\">an unpatterned SiN membrane?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\u001b[1mQuestion: How does the fabrication process of the single-layer pentagonal lattice-based photonic crystal reflector \u001b[0m\n",
       "\u001b[1maffect its thickness and dimensions, and how does this reflector enhance reflectivity and reduce mass compared to \u001b[0m\n",
       "\u001b[1man unpatterned SiN membrane?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: The fabrication process of the single-layer pentagonal lattice-based photonic crystal reflector does </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">affect its thickness and dimensions. According to the information provided, a </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> mm^</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> nm thick, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">single-layer reflector has been fabricated, which is perforated with over a billion nanoscale features. This is the</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">highest aspect-ratio nanophotonic element to date, and it was achieved with nearly </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> times cost reduction per </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">square meter.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The fabrication of this photonic crystal reflector enhances reflectivity and reduces mass compared to an </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">unpatterned SiN membrane. Photonic crystals rely on a two-dimensional array of subwavelength holes in a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">single-layer SiN membrane. The direct relation between the minimum feature size of the patterns allows for the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">enhancement of reflectivity and reduction of mass. This is particularly important for lightsails, which require </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">meter-scale dimensions with nanoscale thickness and billions of nanoscale holes to enhance reflectivity and reduce </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">mass.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">SiN is chosen as the lightsail material due to its low mass, absorption, high reflectivity, and better stability </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">once suspended. The pre-stress in SiN photonic crystals allows for precise alignment of optical beams onto the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">suspended photonic crystals in a lab-scale test setup.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Bilayer photonic crystals can increase thickness and reflection bandwidth, but this trade-off adds mass, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">significantly impacting the sail's acceleration. However, single-layer pentagonal lattice-based photonic crystal </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reflectors offer a solution to increase bandwidth and ease fabrication without adding too much mass.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Source: Pentagonal Photonic Crystal Mirrors: Scalable Lightsails with Enhanced Acceleration via Neural Topology </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Optimization</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: The fabrication process of the single-layer pentagonal lattice-based photonic crystal reflector does \u001b[0m\n",
       "\u001b[1;38;2;118;185;0maffect its thickness and dimensions. According to the information provided, a \u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;38;2;118;185;0m x \u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;38;2;118;185;0m mm^\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;38;2;118;185;0m nm thick, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msingle-layer reflector has been fabricated, which is perforated with over a billion nanoscale features. This is the\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhighest aspect-ratio nanophotonic element to date, and it was achieved with nearly \u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\u001b[1;36m000\u001b[0m\u001b[1;38;2;118;185;0m times cost reduction per \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msquare meter.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe fabrication of this photonic crystal reflector enhances reflectivity and reduces mass compared to an \u001b[0m\n",
       "\u001b[1;38;2;118;185;0munpatterned SiN membrane. Photonic crystals rely on a two-dimensional array of subwavelength holes in a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msingle-layer SiN membrane. The direct relation between the minimum feature size of the patterns allows for the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0menhancement of reflectivity and reduction of mass. This is particularly important for lightsails, which require \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmeter-scale dimensions with nanoscale thickness and billions of nanoscale holes to enhance reflectivity and reduce \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmass.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSiN is chosen as the lightsail material due to its low mass, absorption, high reflectivity, and better stability \u001b[0m\n",
       "\u001b[1;38;2;118;185;0monce suspended. The pre-stress in SiN photonic crystals allows for precise alignment of optical beams onto the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msuspended photonic crystals in a lab-scale test setup.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mBilayer photonic crystals can increase thickness and reflection bandwidth, but this trade-off adds mass, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msignificantly impacting the sail's acceleration. However, single-layer pentagonal lattice-based photonic crystal \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreflectors offer a solution to increase bandwidth and ease fabrication without adding too much mass.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSource: Pentagonal Photonic Crystal Mirrors: Scalable Lightsails with Enhanced Acceleration via Neural Topology \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mOptimization\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "<span style=\"font-weight: bold\">Question: How does the Mistral 7B language model's performance compare to other models such as Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> 13B and </span>\n",
       "<span style=\"font-weight: bold\">Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> 34B? Additionally, how have innovations in neural topology optimization contributed to the development of a</span>\n",
       "<span style=\"font-weight: bold\">scalable and cost-effective solution for lightsail materials in the Starshot Breakthrough Initiative?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\u001b[1mQuestion: How does the Mistral 7B language model's performance compare to other models such as Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m 13B and \u001b[0m\n",
       "\u001b[1mLlama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m 34B? Additionally, how have innovations in neural topology optimization contributed to the development of a\u001b[0m\n",
       "\u001b[1mscalable and cost-effective solution for lightsail materials in the Starshot Breakthrough Initiative?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: The Mistral 7B language model, as reported in a research paper, outperforms other models in its </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">category. According to Figure </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> in the paper, Mistral 7B significantly outperforms Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 7B and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 13B on </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">all evaluated benchmarks. It is also vastly superior to Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 34B in mathematics, code generation, and reasoning </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">benchmarks. This improvement is achieved through the use of grouped-query attention (GQA) for faster inference and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sliding window attention (SWA) to handle sequences of arbitrary length with reduced inference cost.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As for the second part of your question, I'm sorry for any confusion, but the provided document retrieval doesn't </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">contain information about the Starshot Breakthrough Initiative or innovations in neural topology optimization for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">lightsail materials. It's important to ensure that we're using accurate and relevant sources for our answers. If </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">you have a question about a different topic or if you can provide more information, I'd be happy to help.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: The Mistral 7B language model, as reported in a research paper, outperforms other models in its \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcategory. According to Figure \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m in the paper, Mistral 7B significantly outperforms Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m 7B and Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m 13B on \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mall evaluated benchmarks. It is also vastly superior to Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m 34B in mathematics, code generation, and reasoning \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbenchmarks. This improvement is achieved through the use of grouped-query attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mGQA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m for faster inference and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msliding window attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSWA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m to handle sequences of arbitrary length with reduced inference cost.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAs for the second part of your question, I'm sorry for any confusion, but the provided document retrieval doesn't \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcontain information about the Starshot Breakthrough Initiative or innovations in neural topology optimization for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlightsail materials. It's important to ensure that we're using accurate and relevant sources for our answers. If \u001b[0m\n",
       "\u001b[1;38;2;118;185;0myou have a question about a different topic or if you can provide more information, I'd be happy to help.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Generate some synthetic answers to the questions above.\n",
    "##   Try to use the same syntax as the cell above\n",
    "rag_answers = []\n",
    "for i, q in enumerate(synth_questions):\n",
    "    ## TODO: Compute the RAG Answer\n",
    "    rag_answer = \"\"\n",
    "    rag_answer = rag_chain.invoke(q)\n",
    "    rag_answers += [rag_answer]\n",
    "    pprint2(f\"QA Pair {i+1}\", q, \"\", sep=\"\\n\")\n",
    "    pprint(f\"RAG Answer: {rag_answer}\", \"\", sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ho5cnN_Xt_yr",
   "metadata": {
    "id": "Ho5cnN_Xt_yr"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 5:** Implement A Human Preference Metric\n",
    "\n",
    "In this section, we can implement the fourth part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- **Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"**\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sf6f2oFLuPtu",
   "metadata": {
    "id": "sf6f2oFLuPtu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How does the Modular Reasoning, Knowledge and Language (MRKL) system and Bidirectional Encoder </span>\n",
       "<span style=\"font-weight: bold\">Representations from Transformers (BERT) differ in their approach to natural language processing tasks, and what </span>\n",
       "<span style=\"font-weight: bold\">are some of the achievements of BERT compared to previous models?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How does the Modular Reasoning, Knowledge and Language \u001b[0m\u001b[1m(\u001b[0m\u001b[1mMRKL\u001b[0m\u001b[1m)\u001b[0m\u001b[1m system and Bidirectional Encoder \u001b[0m\n",
       "\u001b[1mRepresentations from Transformers \u001b[0m\u001b[1m(\u001b[0m\u001b[1mBERT\u001b[0m\u001b[1m)\u001b[0m\u001b[1m differ in their approach to natural language processing tasks, and what \u001b[0m\n",
       "\u001b[1mare some of the achievements of BERT compared to previous models?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: The MRKL system, as described in the first paper, is a neuro-symbolic architecture that </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">combines large language models, external knowledge sources, and discrete reasoning to overcome the limitations of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language models (LMs). The MRKL system is a flexible architecture with multiple neural models, complemented by </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">discrete knowledge and reasoning modules. It is designed to handle knowledge and reasoning tasks in addition to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">linguistic processing. On the other hand, BERT, described in the second paper, is a conceptually simple yet </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">empirically powerful language representation model that pre-trains deep bidirectional representations from </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">unlabeled text. By jointly conditioning on both left and right context in all layers, the pre-trained BERT model </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">can be fine-tuned to create state-of-the-art models for various tasks such as question answering and language </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">inference without substantial task-specific architecture modifications. BERT has achieved new state-of-the-art </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">results on eleven natural language processing tasks, including improvements on the GLUE score, MultiNLI accuracy, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and SQuAD question answering.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: The MRKL system, as described in the first paper, is a neuro-symbolic architecture that \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcombines large language models, external knowledge sources, and discrete reasoning to overcome the limitations of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. The MRKL system is a flexible architecture with multiple neural models, complemented by \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdiscrete knowledge and reasoning modules. It is designed to handle knowledge and reasoning tasks in addition to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlinguistic processing. On the other hand, BERT, described in the second paper, is a conceptually simple yet \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mempirically powerful language representation model that pre-trains deep bidirectional representations from \u001b[0m\n",
       "\u001b[1;38;2;118;185;0munlabeled text. By jointly conditioning on both left and right context in all layers, the pre-trained BERT model \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcan be fine-tuned to create state-of-the-art models for various tasks such as question answering and language \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minference without substantial task-specific architecture modifications. BERT has achieved new state-of-the-art \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mresults on eleven natural language processing tasks, including improvements on the GLUE score, MultiNLI accuracy, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand SQuAD question answering.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: The Modular Reasoning, Knowledge and Language (MRKL) system and Bidirectional Encoder Representations </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">from Transformers (BERT) both play significant roles in natural language processing tasks but use different </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">approaches.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">BERT, as per the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, is a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language representation model that uses bidirectional transformers. This means it considers the context on both </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sides of a word, resulting in a deeper sense of language context. It's designed to pre-train deep bidirectional </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">representations from unlabeled text, which can then be fine-tuned with just one additional output layer to create </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art models for various tasks without significant task-specific architecture modifications. BERT has </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">achieved several milestones, including new state-of-the-art results on eleven natural language processing tasks. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">For example, it improved the GLUE score to </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80.5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% and pushed the SQuAD v1.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> question answering Test F1 to </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93.2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">On the other hand, the MRKL system is a neuro-symbolic architecture that combines large language models, such as </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">BERT, with external knowledge sources and discrete reasoning. The MRKL system aims to extend the capabilities of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language models by integrating symbolic reasoning and external knowledge, allowing it to solve tasks that BERT </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">alone cannot. It's worth noting that while BERT is a specific model, MRKL is more of an architectural pattern that </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">can use BERT as one of its components.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">These differences reflect a broader trend in AI research towards integrating neural and symbolic approaches to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">achieve more robust and versatile systems. It's also interesting to see how models like BERT are being incorporated</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">into these broader architectures like MRKL.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In summary, BERT represents a powerful pretrained language model that has achieved remarkable results in a variety </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of NLP tasks. MRKL, meanwhile, is a broader architectural concept that seeks to leverage the strengths of models </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">like BERT along with external knowledge and reasoning to tackle even more challenging tasks.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sources:</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sources and discrete reasoning]</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: The Modular Reasoning, Knowledge and Language \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMRKL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m system and Bidirectional Encoder Representations \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfrom Transformers \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mBERT\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m both play significant roles in natural language processing tasks but use different \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mapproaches.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mBERT, as per the paper \u001b[0m\u001b[32m\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"\u001b[0m\u001b[1;38;2;118;185;0m, is a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage representation model that uses bidirectional transformers. This means it considers the context on both \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msides of a word, resulting in a deeper sense of language context. It's designed to pre-train deep bidirectional \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrepresentations from unlabeled text, which can then be fine-tuned with just one additional output layer to create \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstate-of-the-art models for various tasks without significant task-specific architecture modifications. BERT has \u001b[0m\n",
       "\u001b[1;38;2;118;185;0machieved several milestones, including new state-of-the-art results on eleven natural language processing tasks. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mFor example, it improved the GLUE score to \u001b[0m\u001b[1;36m80.5\u001b[0m\u001b[1;38;2;118;185;0m% and pushed the SQuAD v1.\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m question answering Test F1 to \u001b[0m\u001b[1;36m93.2\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOn the other hand, the MRKL system is a neuro-symbolic architecture that combines large language models, such as \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mBERT, with external knowledge sources and discrete reasoning. The MRKL system aims to extend the capabilities of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage models by integrating symbolic reasoning and external knowledge, allowing it to solve tasks that BERT \u001b[0m\n",
       "\u001b[1;38;2;118;185;0malone cannot. It's worth noting that while BERT is a specific model, MRKL is more of an architectural pattern that \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcan use BERT as one of its components.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThese differences reflect a broader trend in AI research towards integrating neural and symbolic approaches to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0machieve more robust and versatile systems. It's also interesting to see how models like BERT are being incorporated\u001b[0m\n",
       "\u001b[1;38;2;118;185;0minto these broader architectures like MRKL.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn summary, BERT represents a powerful pretrained language model that has achieved remarkable results in a variety \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mof NLP tasks. MRKL, meanwhile, is a broader architectural concept that seeks to leverage the strengths of models \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlike BERT along with external knowledge and reasoning to tackle even more challenging tasks.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSources:\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mMRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msources and discrete reasoning\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">] The second answer is better than the first and does not introduce any inconsistencies.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Justification:</span>\n",
       "<span style=\"font-weight: bold\">The second answer provides a more detailed and understandable explanation of the differences between the MRKL </span>\n",
       "<span style=\"font-weight: bold\">system and BERT. It clearly explains that BERT is a bidirectional language representation model and summarizes its </span>\n",
       "<span style=\"font-weight: bold\">achievements, including its new state-of-the-art results on eleven natural language processing tasks. The second </span>\n",
       "<span style=\"font-weight: bold\">answer also describes the MRKL system as a neuro-symbolic architecture that combines language models, external </span>\n",
       "<span style=\"font-weight: bold\">knowledge sources, and discrete reasoning, clarifying that it is more of an architectural pattern that can use BERT</span>\n",
       "<span style=\"font-weight: bold\">as one of its components. Furthermore, the second answer establishes the broader trend of integrating neural and </span>\n",
       "<span style=\"font-weight: bold\">symbolic approaches, setting the context for the comparison between the two models. It also maintains consistency </span>\n",
       "<span style=\"font-weight: bold\">with the information provided in the first answer and does not introduce any contradictions. Overall, the second </span>\n",
       "<span style=\"font-weight: bold\">answer is more comprehensive and accessible, making it a better explanation of the subject matter.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m The second answer is better than the first and does not introduce any inconsistencies.\u001b[0m\n",
       "\n",
       "\u001b[1mJustification:\u001b[0m\n",
       "\u001b[1mThe second answer provides a more detailed and understandable explanation of the differences between the MRKL \u001b[0m\n",
       "\u001b[1msystem and BERT. It clearly explains that BERT is a bidirectional language representation model and summarizes its \u001b[0m\n",
       "\u001b[1machievements, including its new state-of-the-art results on eleven natural language processing tasks. The second \u001b[0m\n",
       "\u001b[1manswer also describes the MRKL system as a neuro-symbolic architecture that combines language models, external \u001b[0m\n",
       "\u001b[1mknowledge sources, and discrete reasoning, clarifying that it is more of an architectural pattern that can use BERT\u001b[0m\n",
       "\u001b[1mas one of its components. Furthermore, the second answer establishes the broader trend of integrating neural and \u001b[0m\n",
       "\u001b[1msymbolic approaches, setting the context for the comparison between the two models. It also maintains consistency \u001b[0m\n",
       "\u001b[1mwith the information provided in the first answer and does not introduce any contradictions. Overall, the second \u001b[0m\n",
       "\u001b[1manswer is more comprehensive and accessible, making it a better explanation of the subject matter.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How does the fabrication process of the single-layer pentagonal lattice-based photonic crystal </span>\n",
       "<span style=\"font-weight: bold\">reflector affect its thickness and dimensions, and how does this reflector enhance reflectivity and reduce mass </span>\n",
       "<span style=\"font-weight: bold\">compared to an unpatterned SiN membrane?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How does the fabrication process of the single-layer pentagonal lattice-based photonic crystal \u001b[0m\n",
       "\u001b[1mreflector affect its thickness and dimensions, and how does this reflector enhance reflectivity and reduce mass \u001b[0m\n",
       "\u001b[1mcompared to an unpatterned SiN membrane?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: The single-layer pentagonal lattice-based photonic crystal reflector is fabricated using a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">process that involves spin coating, soft baking, exposure, development, and etching steps. The fabricated reflector</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">has dimensions of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> mm^</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> with a thickness of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> nm. It is perforated with over a billion nanoscale features,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">which enhances reflectivity and reduces mass compared to an unpatterned SiN membrane. Specifically, simulations </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">have shown that the pentagonal lattice design achieves a reflectivity of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% for an Area fraction of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">55</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% and a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">thickness of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.18</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> µm, compared to a </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% reflective and </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> nm thick unpatterned SiN membrane. This is in agreement </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">with theoretical predictions.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: The single-layer pentagonal lattice-based photonic crystal reflector is fabricated using a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprocess that involves spin coating, soft baking, exposure, development, and etching steps. The fabricated reflector\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhas dimensions of \u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;38;2;118;185;0m x \u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;38;2;118;185;0m mm^\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m with a thickness of \u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;38;2;118;185;0m nm. It is perforated with over a billion nanoscale features,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwhich enhances reflectivity and reduces mass compared to an unpatterned SiN membrane. Specifically, simulations \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhave shown that the pentagonal lattice design achieves a reflectivity of \u001b[0m\u001b[1;36m84\u001b[0m\u001b[1;38;2;118;185;0m% for an Area fraction of \u001b[0m\u001b[1;36m55\u001b[0m\u001b[1;38;2;118;185;0m% and a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthickness of \u001b[0m\u001b[1;36m0.18\u001b[0m\u001b[1;38;2;118;185;0m µm, compared to a \u001b[0m\u001b[1;36m35\u001b[0m\u001b[1;38;2;118;185;0m% reflective and \u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;38;2;118;185;0m nm thick unpatterned SiN membrane. This is in agreement \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwith theoretical predictions.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: The fabrication process of the single-layer pentagonal lattice-based photonic crystal reflector does </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">affect its thickness and dimensions. According to the information provided, a </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> mm^</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> nm thick, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">single-layer reflector has been fabricated, which is perforated with over a billion nanoscale features. This is the</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">highest aspect-ratio nanophotonic element to date, and it was achieved with nearly </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> times cost reduction per </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">square meter.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The fabrication of this photonic crystal reflector enhances reflectivity and reduces mass compared to an </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">unpatterned SiN membrane. Photonic crystals rely on a two-dimensional array of subwavelength holes in a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">single-layer SiN membrane. The direct relation between the minimum feature size of the patterns allows for the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">enhancement of reflectivity and reduction of mass. This is particularly important for lightsails, which require </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">meter-scale dimensions with nanoscale thickness and billions of nanoscale holes to enhance reflectivity and reduce </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">mass.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">SiN is chosen as the lightsail material due to its low mass, absorption, high reflectivity, and better stability </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">once suspended. The pre-stress in SiN photonic crystals allows for precise alignment of optical beams onto the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">suspended photonic crystals in a lab-scale test setup.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Bilayer photonic crystals can increase thickness and reflection bandwidth, but this trade-off adds mass, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">significantly impacting the sail's acceleration. However, single-layer pentagonal lattice-based photonic crystal </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reflectors offer a solution to increase bandwidth and ease fabrication without adding too much mass.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Source: Pentagonal Photonic Crystal Mirrors: Scalable Lightsails with Enhanced Acceleration via Neural Topology </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Optimization</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: The fabrication process of the single-layer pentagonal lattice-based photonic crystal reflector does \u001b[0m\n",
       "\u001b[1;38;2;118;185;0maffect its thickness and dimensions. According to the information provided, a \u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;38;2;118;185;0m x \u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;38;2;118;185;0m mm^\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;38;2;118;185;0m nm thick, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msingle-layer reflector has been fabricated, which is perforated with over a billion nanoscale features. This is the\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhighest aspect-ratio nanophotonic element to date, and it was achieved with nearly \u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\u001b[1;36m000\u001b[0m\u001b[1;38;2;118;185;0m times cost reduction per \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msquare meter.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe fabrication of this photonic crystal reflector enhances reflectivity and reduces mass compared to an \u001b[0m\n",
       "\u001b[1;38;2;118;185;0munpatterned SiN membrane. Photonic crystals rely on a two-dimensional array of subwavelength holes in a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msingle-layer SiN membrane. The direct relation between the minimum feature size of the patterns allows for the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0menhancement of reflectivity and reduction of mass. This is particularly important for lightsails, which require \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmeter-scale dimensions with nanoscale thickness and billions of nanoscale holes to enhance reflectivity and reduce \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmass.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSiN is chosen as the lightsail material due to its low mass, absorption, high reflectivity, and better stability \u001b[0m\n",
       "\u001b[1;38;2;118;185;0monce suspended. The pre-stress in SiN photonic crystals allows for precise alignment of optical beams onto the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msuspended photonic crystals in a lab-scale test setup.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mBilayer photonic crystals can increase thickness and reflection bandwidth, but this trade-off adds mass, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msignificantly impacting the sail's acceleration. However, single-layer pentagonal lattice-based photonic crystal \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreflectors offer a solution to increase bandwidth and ease fabrication without adding too much mass.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSource: Pentagonal Photonic Crystal Mirrors: Scalable Lightsails with Enhanced Acceleration via Neural Topology \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mOptimization\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">] Justification: While Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> does provide valuable information regarding the fabrication </span>\n",
       "<span style=\"font-weight: bold\">process of the single-layer pentagonal lattice-based photonic crystal reflector, it does not directly address how </span>\n",
       "<span style=\"font-weight: bold\">the fabrication process affects the thickness and dimensions of the reflector as clearly as Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">. Additionally,</span>\n",
       "<span style=\"font-weight: bold\">Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> provides specific simulation results and theoretical predictions about the reflector's reflectivity and </span>\n",
       "<span style=\"font-weight: bold\">mass reduction compared to an unpatterned SiN membrane, whereas Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> discusses these aspects more generally </span>\n",
       "<span style=\"font-weight: bold\">without providing the precise figures that Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> does. Therefore, Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> is more consistent with the question </span>\n",
       "<span style=\"font-weight: bold\">and provides a more detailed and accurate response.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m Justification: While Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m does provide valuable information regarding the fabrication \u001b[0m\n",
       "\u001b[1mprocess of the single-layer pentagonal lattice-based photonic crystal reflector, it does not directly address how \u001b[0m\n",
       "\u001b[1mthe fabrication process affects the thickness and dimensions of the reflector as clearly as Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m. Additionally,\u001b[0m\n",
       "\u001b[1mAnswer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m provides specific simulation results and theoretical predictions about the reflector's reflectivity and \u001b[0m\n",
       "\u001b[1mmass reduction compared to an unpatterned SiN membrane, whereas Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m discusses these aspects more generally \u001b[0m\n",
       "\u001b[1mwithout providing the precise figures that Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m does. Therefore, Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m is more consistent with the question \u001b[0m\n",
       "\u001b[1mand provides a more detailed and accurate response.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How does the Mistral 7B language model's performance compare to other models such as Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> </span>\n",
       "<span style=\"font-weight: bold\">13B and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> 34B? Additionally, how have innovations in neural topology optimization contributed to the </span>\n",
       "<span style=\"font-weight: bold\">development of a scalable and cost-effective solution for lightsail materials in the Starshot Breakthrough </span>\n",
       "<span style=\"font-weight: bold\">Initiative?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How does the Mistral 7B language model's performance compare to other models such as Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m \u001b[0m\n",
       "\u001b[1m13B and Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m 34B? Additionally, how have innovations in neural topology optimization contributed to the \u001b[0m\n",
       "\u001b[1mdevelopment of a scalable and cost-effective solution for lightsail materials in the Starshot Breakthrough \u001b[0m\n",
       "\u001b[1mInitiative?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: The Mistral 7B language model outperforms the best open 13B model (Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) across all </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">evaluated benchmarks and the best released 34B model (Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) in reasoning, mathematics, and code generation. The </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mistral 7B uses grouped-query attention for faster inference and sliding window attention for effectively handling </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sequences of arbitrary length with reduced inference cost. In the context of the Starshot Breakthrough Initiative, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">neural topology optimization has led to the development of a novel pentagonal lattice-based photonic crystal </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reflector for lightsails. These optimized designs not only shorten acceleration times, lowering launch costs, but </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">also enable lightsail material fabrication with orders-of-magnitude cost reduction. A </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> mm2, 200nm thick, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">single-layer reflector has been fabricated with over a billion nanoscale features, achieving nearly </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> times </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">cost reduction per m2, making it the highest aspect-ratio nanophotonic element to date.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: The Mistral 7B language model outperforms the best open 13B model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLlama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m across all \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mevaluated benchmarks and the best released 34B model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLlama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m in reasoning, mathematics, and code generation. The \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMistral 7B uses grouped-query attention for faster inference and sliding window attention for effectively handling \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msequences of arbitrary length with reduced inference cost. In the context of the Starshot Breakthrough Initiative, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mneural topology optimization has led to the development of a novel pentagonal lattice-based photonic crystal \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreflector for lightsails. These optimized designs not only shorten acceleration times, lowering launch costs, but \u001b[0m\n",
       "\u001b[1;38;2;118;185;0malso enable lightsail material fabrication with orders-of-magnitude cost reduction. A \u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;38;2;118;185;0m x \u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;38;2;118;185;0m mm2, 200nm thick, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msingle-layer reflector has been fabricated with over a billion nanoscale features, achieving nearly \u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\u001b[1;36m000\u001b[0m\u001b[1;38;2;118;185;0m times \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcost reduction per m2, making it the highest aspect-ratio nanophotonic element to date.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: The Mistral 7B language model, as reported in a research paper, outperforms other models in its </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">category. According to Figure </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> in the paper, Mistral 7B significantly outperforms Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 7B and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 13B on </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">all evaluated benchmarks. It is also vastly superior to Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 34B in mathematics, code generation, and reasoning </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">benchmarks. This improvement is achieved through the use of grouped-query attention (GQA) for faster inference and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sliding window attention (SWA) to handle sequences of arbitrary length with reduced inference cost.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As for the second part of your question, I'm sorry for any confusion, but the provided document retrieval doesn't </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">contain information about the Starshot Breakthrough Initiative or innovations in neural topology optimization for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">lightsail materials. It's important to ensure that we're using accurate and relevant sources for our answers. If </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">you have a question about a different topic or if you can provide more information, I'd be happy to help.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: The Mistral 7B language model, as reported in a research paper, outperforms other models in its \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcategory. According to Figure \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m in the paper, Mistral 7B significantly outperforms Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m 7B and Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m 13B on \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mall evaluated benchmarks. It is also vastly superior to Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m 34B in mathematics, code generation, and reasoning \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbenchmarks. This improvement is achieved through the use of grouped-query attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mGQA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m for faster inference and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msliding window attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSWA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m to handle sequences of arbitrary length with reduced inference cost.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAs for the second part of your question, I'm sorry for any confusion, but the provided document retrieval doesn't \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcontain information about the Starshot Breakthrough Initiative or innovations in neural topology optimization for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlightsail materials. It's important to ensure that we're using accurate and relevant sources for our answers. If \u001b[0m\n",
       "\u001b[1;38;2;118;185;0myou have a question about a different topic or if you can provide more information, I'd be happy to help.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">] Justification: The second answer is not inferior but is incomplete compared to the first </span>\n",
       "<span style=\"font-weight: bold\">answer. While Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> provides detailed information on the performance of the Mistral 7B language model, it fails </span>\n",
       "<span style=\"font-weight: bold\">to address the second part of the question regarding the Starshot Breakthrough Initiative and innovations in neural</span>\n",
       "<span style=\"font-weight: bold\">topology optimization. In contrast, the first answer provides a comprehensive response to both parts of the </span>\n",
       "<span style=\"font-weight: bold\">question, making it the superior answer.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m Justification: The second answer is not inferior but is incomplete compared to the first \u001b[0m\n",
       "\u001b[1manswer. While Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m provides detailed information on the performance of the Mistral 7B language model, it fails \u001b[0m\n",
       "\u001b[1mto address the second part of the question regarding the Starshot Breakthrough Initiative and innovations in neural\u001b[0m\n",
       "\u001b[1mtopology optimization. In contrast, the first answer provides a comprehensive response to both parts of the \u001b[0m\n",
       "\u001b[1mquestion, making it the superior answer.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_prompt = ChatPromptTemplate.from_template(\"\"\"INSTRUCTION: \n",
    "Evaluate the following Question-Answer pair for human preference and consistency.\n",
    "Assume the first answer is a ground truth answer and has to be correct.\n",
    "Assume the second answer may or may not be true.\n",
    "[1] The second answer lies, does not answer the question, or is inferior to the first answer.\n",
    "[2] The second answer is better than the first and does not introduce any inconsistencies.\n",
    "\n",
    "Output Format:\n",
    "[Score] Justification\n",
    "\n",
    "{qa_trio}\n",
    "\n",
    "EVALUATION: \n",
    "\"\"\")\n",
    "\n",
    "pref_score = []\n",
    "\n",
    "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
    "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
    "    pprint2(f\"Set {i+1}\\n\\nQuestion: {q}\\n\\n\")\n",
    "\n",
    "    qa_trio = f\"Question: {q}\\n\\nAnswer 1 (Ground Truth): {a_synth}\\n\\n Answer 2 (New Answer): {a_rag}\"\n",
    "    pref_score += [(eval_prompt | llm).invoke({'qa_trio': qa_trio})]\n",
    "    pprint(f\"Synth Answer: {a_synth}\\n\\n\")\n",
    "    pprint(f\"RAG Answer: {a_rag}\\n\\n\")\n",
    "    pprint2(f\"Synth Evaluation: {pref_score[-1]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6595662-9f49-44eb-9868-2a3fdb1fb60f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Congratulations! We now have an LLM system that reasons about our pipeline and tries to evaluate it!** Now that we have some judge results, we can simply aggregate the results and see how often our formulation was according to an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3L_q6fMH3i6_",
   "metadata": {
    "id": "3L_q6fMH3i6_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference Score: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "pref_score = sum((\"[2]\" in score) for score in pref_score) / len(pref_score)\n",
    "print(f\"Preference Score: {pref_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80bf04-118d-44a2-a740-361a756a1d5f",
   "metadata": {
    "id": "cf80bf04-118d-44a2-a740-361a756a1d5f"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4:** Advanced Formulations\n",
    "\n",
    "The exercise above was meant to prepare you for the final assessment of the course and showcased a simple but effective evaluator chain. The objective and implementation details were provided for you, and the logic for using it probably makes sense now that you've seen it in action. \n",
    "\n",
    "With that being said, this metric was merely a product of us specifying:\n",
    "- **What kind of behavior is important for our pipeline to have?**\n",
    "- **What do we need to do in order to exhibit and evaluate this behavior?**\n",
    "\n",
    "From these two questions, we could have come up with plenty of other evaluation metrics that could have assessed different attributes, incorporated different evaluator chain techniques, and even required different pipeline organization strategies. Though far from an exhaustive list, some common formulations you will likely come across may include:\n",
    "\n",
    "- **Style Evaluation:** Some evaluation formulations can be as simple as \"let me ask some questions and see if the output feels desirable.\" This might be used to see whether a chatbot \"acts like it's supposed to\" based on a description provided to a judge LLM. We're using quotations since this kind of assessment can reasonably be achieved with nothing but prompt engineering and a while loop.\n",
    "\n",
    "- **Ground-Truth Evaluation:** In our chain, we used synthetic generation to create some random questions and answers using a sampling strategy, but in reality you may actually have some representative questions and answers that you need your chatbot to consistently get right! In this case, a modification of the exercise chain above should be implemented and closely monitored as you develop your pipelines.\n",
    "\n",
    "- **Retrieval/Augmentation Evaluation:** This course made many assumptions about what kinds of preprocessing and prompting steps would be good for your pipelines, and much of this was determined by experimentation. Factors such as document preprocessing, chunking strategies, model selection, and prompt specification all played important roles, so creating metrics to validate these decisions may be of interest. This kind of metric might require your pipeline to output your context chunks or may even rely solely on embedding similarity comparisons, so keep this in mind when trying to implement a chain that works with multiple evaluation strategies. Consider the [**RagasEvaluatorChain**](https://docs.ragas.io/en/latest/howtos/integrations/langchain.html) abstraction as a decent starting point for making an custom generalizable evaluation routine. \n",
    "\n",
    "- **Trajectory Evaluation:** Using more advanced agent formulations, you can implement multiple-query strategies that assume the presence of conversational memory. With this, you can implement an evaluation agent which can:\n",
    "    - Ask a series of questions in order to evaluate how well the agent is able to adapt and cater to the scenario. This kind of system generally considers a series of correspondence and aims to tease out and evaluate a \"trajectory\" of how the agent navigated the conversation. The [**LangChain Trajectory Evaluators documentation**](https://python.langchain.com/docs/guides/evaluation/trajectory/) is a good starting point.\n",
    "    - Alternatively, you could also implement an evaluation agent that tries to achieve objectives by interacting with the chatbot. Such an agent can output whether they were able to navigate to their solution in a natural manner, and can even be used to generate a report about the percieved performance. The [**LangChain Agents documentation**](https://python.langchain.com/docs/modules/agents/concepts) is a good starting point!\n",
    "\n",
    "<br>\n",
    "\n",
    "At the end of the day, just make sure to use the tools you have at your disposal appropriately. By this point in the course, you should already be well-acquainted with the LLM core value propositions: **They're powerful, scalable, predictable, controllable, and orchestratable... but will act unpredictably when you just expect them to work by default.** Assess your needs, formulate and validate your pipelines, give enough information, and add as much control as you can to make your system work consistently, efficiently, and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61faee2c-e534-4c89-91ae-45c37835dba5",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5: [Assessment]** Evaluating For Credit\n",
    "\n",
    "Welcome to the last exercise of the course! Hopefully you've enjoyed the material and are ready to actually get credit for these notebooks! For this part:\n",
    "\n",
    "- **Make sure you're in the course environment**\n",
    "- **Make sure `docstore_index/` has been uploaded to the course environment...**\n",
    "    - **...and contains [at least one Arxiv paper](https://arxiv.org/search/advanced) which has been updated recently.**\n",
    "- **Make sure [`35_langserve.ipynb`](35_langserve.ipynb) is not occupying port 9012 with a running FastAPI service**\n",
    "\n",
    "**Objective:** On launch, [**`frontend/frontend_block.py`**](frontend/frontend_block.py) had several lines of code which trigger the course pass condition. Once the service was deemed healthy, that sequence of code was replaced with `## secret` by another microservice. Your objective is to invoke that series of commands by using your pipeline to pass the **Evaluation** check! Recall [`35_langserve.ipynb`](35_langserve.ipynb) and use it as a starting example! As a recommendation, consider duplicating it so that you can keep the original as an authoritative reference. \n",
    "\n",
    "**Once Finished:** While your course environment is still open, please navigate back to your course environment launcher area and click the **\"Assess Task\"** button! After that, you're all done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48e300ed-951c-4006-ac54-cbbd41251707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var url = 'http://'+window.location.host+'/8090';\n",
       "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+'/8090';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0",
   "metadata": {
    "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## <font color=\"#76b900\">**Congratulations On Completing The Course**</font>\n",
    "\n",
    "Hopefully this course was not only exciting and challenging, but also adequately prepared you for work on the cutting edge of LLM and RAG system development! Going forward, you should have the skills necessary to tackle industry-level challenges and explore RAG deployment with open-source models and frameworks.\n",
    "\n",
    "**Some NVIDIA-specific releases related to this that you may find interesting include:**\n",
    "- [**NVIDIA NIMs**](https://www.nvidia.com/en-us/ai/), which offers microservice spinup routines that can be deployed on local compute.\n",
    "- [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) is the current recommended framework for deploying GPU-accelerated LLM model engines in production settings.\n",
    "- [**NVIDIA's Generative AI Examples Repo**](https://github.com/NVIDIA/GenerativeAIExamples), which includes the current canonical microservice example application and will be updated with new resources as new production workflows get released.\n",
    "- [**The Knowledge-Based Chatbot Technical Brief**](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/knowledge-base-chatbot-technical-brief) which discusses additional publicly-accessible details on productionalizing RAG systems.\n",
    "\n",
    "**Additionally, some key topics you may be interested in delving more into include:**\n",
    "- [**LlamaIndex**](https://www.llamaindex.ai/), which has strong components that can augment and occasionally improve upon the LangChain RAG features.\n",
    "- [**LangSmith**](https://docs.smith.langchain.com/), an upcoming agent productionalization service offered by LangChain.\n",
    "- [**Gradio**](https://www.gradio.app/), though touched on in the course, has many more interface options which will be worth investigating. For inspiration, consider checking out [**HuggingFace Spaces**](https://huggingface.co/spaces) for examples.\n",
    "- [**LangGraph**](https://python.langchain.com/docs/langgraph/) is a framework for graph-based LLM orchestration, and is a natural next step forward for those interested in [multi-agent workflows](https://blog.langchain.dev/langgraph-multi-agent-workflows/).\n",
    "- [**DSPy**](https://github.com/stanfordnlp/dspy), a flow engineering framework that allows you to optimize LLM orchestration pipelines based on empirical performance results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035451c9-ed12-4bc3-b468-04db5c399e03",
   "metadata": {
    "id": "035451c9-ed12-4bc3-b468-04db5c399e03"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
